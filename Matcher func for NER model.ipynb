{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc70cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL CODE:\n",
    "#Importing libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.util import filter_spans\n",
    "from spacy import displacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Span\n",
    "\n",
    "tqdm.pandas()\n",
    "#using pretrained gensim model for creating vocabulary\n",
    "import gensim.downloader as api\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084ee5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf928019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading different embeddings function by choice #files located locally\n",
    "def load_embed(embed):\n",
    "    if embed == \"fasttext\":\n",
    "        return  (KeyedVectors.load('saved_ft_embed',mmap='r'))\n",
    "    elif embed == \"glove\":\n",
    "        return  (KeyedVectors.load('saved_glove_embed',mmap='r'))\n",
    "    elif embed == \"conceptnet\":\n",
    "        #return (KeyedVectors.load(r'C:\\Users\\new\\Documents\\PythonFiles\\conceptnet_embeddings\\saved_numb_batch_embed',mmap='r'))\n",
    "        return (KeyedVectors.load(r\"C:\\Users\\DELL\\Desktop\\NLP files\\Ner model files\\conceptnet_embeddings\\saved_numb_batch_embed\",mmap='r'))\n",
    "    else:   \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce5a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function loads and preprocesses the corpus and the vocabulary\n",
    "def LoadAndPreprocessInputs(corpus_filename, vocab_filename = None, keyword_list = None):\n",
    "    print(\"preprocess functions is runnning\")\n",
    "    # Readin the corpus file\n",
    "    corpus_data = open(corpus_filename, encoding='utf-8').read().strip()\n",
    "\n",
    "    # split into patents texts | 1 entry = 1 patent\n",
    "    #corpus_texts = corpus_data.split('\\n\\n')\n",
    "\n",
    "    # split each patent into lines\n",
    "    corpus_lines = corpus_data.split('\\n')\n",
    "    print(len(corpus_lines))\n",
    "#     print(len(corpus_texts))\n",
    "#     print(type(corpus_texts))\n",
    "    #create our own vocublary using key words\n",
    "        #-------------Need to implement---------------\n",
    "        \n",
    "    if vocab_filename.endswith('json'):\n",
    "        f = open(vocab_filename)\n",
    "        vocab = json.load(f)\n",
    "        new_vocab = {}\n",
    "\n",
    "        return corpus_lines, vocab#vocab_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0a3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_comparision(LABEL_NAME, line, nlp, embed_model, keyword_embedding, matcher_dict, keyword, line_index, updated_vocabulary):\n",
    "\n",
    "    doc = nlp(line)\n",
    "    chunk_dict = {}\n",
    "    counter = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "\n",
    "        if str(chunk.root.text) in embed_model.key_to_index:\n",
    "            similarity = get_cosine_similarity(keyword_embedding, embed_model[str(chunk.root.text)])\n",
    "            if similarity > 0.6:\n",
    "                # print(\"\\tLine index: \", line_index)\n",
    "                # print(\"\\tLine: \",line)\n",
    "                # print(\"\\t\\t keyword: \",keyword,\"   chunk: \", chunk,\" chunk root word: \",str(chunk.root.text),\" similarity: \",similarity)\n",
    "                #pattern = nlp.make_doc(str(chunk))\n",
    "                matcher_dict[LABEL_NAME].append(str(chunk))\n",
    "                updated_vocabulary[LABEL_NAME].append(str(chunk))\n",
    "                counter+=1\n",
    "    return matcher_dict, counter, updated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76eda3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_comparision(LABEL_NAME, line, nlp, keyword, matcher_dict, line_index):\n",
    "    \n",
    "    match =re.search(\"\\W*(\"+keyword+\")\\W*\",line)\n",
    "    \n",
    "    if match != None:\n",
    "        # print(\"\\t\\tLine index: \", line_index)\n",
    "        # print(\"\\t\\tLine: \",line)\n",
    "        # print(\"\\t\\t\\t keyword: \",keyword, '--- match :', match.group())\n",
    "        #pattern = nlp.make_doc(str(match.group()))\n",
    "        matcher_dict[LABEL_NAME].append(str(match.group()))\n",
    "    return matcher_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8564b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_with_embed(vocab_processed, text_lines, embed_model, nlp):\n",
    "    matcher_dict = {}   \n",
    "    # f_time = datetime.datetime.now()  #for saving file\n",
    "    # file_time = f_time.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "    # file_name = \"matcher_trial_\"+file_time+\".json\"\n",
    "    counter1 = 0\n",
    "    updated_vocabulary = copy.deepcopy(vocab_processed)\n",
    "    for LABEL_NAME in vocab_processed: #iterating through vocab\n",
    "        # start_time = datetime.datetime.now()\n",
    "        # matcher_dict[\"start_time\"]=start_time.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "        print(\"Currently processing keyword :\", LABEL_NAME)\n",
    "        matcher_dict[LABEL_NAME] = []\n",
    "        expanded_keywords = vocab_processed[LABEL_NAME]\n",
    "        \n",
    "        for keyword in list(expanded_keywords)[0:50]:\n",
    "            _keyword = keyword.replace(' ','_')\n",
    "            #print(\"the expanded word currently running is :\", word)\n",
    "            \n",
    "#             print(\"\\t------------------------------------\")\n",
    "#             print(\"\\tWordnet keywords: \",keyword)\n",
    "#            print(len(text_lines))\n",
    "            for line_index, line in enumerate(text_lines): #iterating through corpus lines\n",
    "                counter1=counter1+1\n",
    "                if(counter1%1000 == 0):\n",
    "                    print(f'Process is running -- current iteration is...{counter1}', end='\\r')\n",
    "                    #print(\"Process is running -- current iteration is...\", counter1)\n",
    "                \n",
    "                #keyword_embedding=[]#np.zeros(300,)\n",
    "                if _keyword in embed_model.key_to_index: #if only present in vocab \n",
    "                    #print('_keyword checking -->',_keyword)\n",
    "                    keyword_embedding = embed_model[_keyword]\n",
    "                    matcher_dict, counter, updated_vocabulary = chunk_comparision(LABEL_NAME, line, nlp, embed_model, keyword_embedding, matcher_dict, keyword, line_index, updated_vocabulary)\n",
    "                    if counter == 0:\n",
    "                        matcher_dict = string_comparision(LABEL_NAME, line, nlp, keyword, matcher_dict, line_index)\n",
    "                else:\n",
    "                    mather_dict = string_comparision(LABEL_NAME, line, nlp, keyword, matcher_dict, line_index)\n",
    "                \n",
    "            \n",
    "            \n",
    "        matcher_dict[LABEL_NAME] = list(set(matcher_dict[LABEL_NAME]))\n",
    "        # end_time = datetime.datetime.now()\n",
    "        # matcher_dict[\"end_time\"]=end_time.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "         \n",
    "        # f =open(file_name,'a')        #Saving file for checking the vocab\n",
    "        # f.write(json.dumps(list(set(matcher_dict[LABEL_NAME])))+\"\\n\")\n",
    "        # f.close()\n",
    "        print(\"Keyword completed: \",LABEL_NAME)\n",
    "        \n",
    "    return matcher_dict, updated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191250ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processess Matcher and also splits train and test data.\n",
    "def MatcherForGeneratingTrainData(vocab_processed, corpus_lines, embed_model):\n",
    "    print(\"matcher func is running\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    except:\n",
    "        !python -m spacy download en_core_web_lg\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    matcher_dict, updated_vocabulary = matching_with_embed(vocab_processed, corpus_lines, embed_model, nlp)\n",
    "    return matcher_dict, updated_vocabulary\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")   #initializing the matcher\n",
    "    for keyword, patterns in matcher_dict.items():\n",
    "        matcher.add(keyword, patterns)\n",
    "      \n",
    "    #splitting of corpus into train and test is done:\n",
    "    train_lines, test_lines = train_test_split(corpus_lines, test_size=0.3, random_state=42)\n",
    "    return matcher, train_lines, test_lines, nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6f3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomNerModelMainFunc(corpus_filename, vocab_filename, testing_input = None):\n",
    "    embed_model = load_embed('conceptnet')\n",
    "    corpus_lines, vocab_processed = LoadAndPreprocessInputs(corpus_filename, vocab_filename)\n",
    "    matcher_dict, updated_vocabulary = MatcherForGeneratingTrainData(vocab_processed, corpus_lines, embed_model)\n",
    "    return matcher_dict, updated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "564655ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#started at 12.29 p.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5066f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess functions is runnning\n",
      "12833\n",
      "matcher func is running\n",
      "Currently processing keyword : INVESTMENT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_cosine_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f63209a514ea>\u001b[0m in \u001b[0;36mCustomNerModelMainFunc\u001b[1;34m(corpus_filename, vocab_filename, testing_input)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0membed_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conceptnet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcorpus_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadAndPreprocessInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatcherForGeneratingTrainData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-ceeae225e924>\u001b[0m in \u001b[0;36mMatcherForGeneratingTrainData\u001b[1;34m(vocab_processed, corpus_lines, embed_model)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python -m spacy download en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_lg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatching_with_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LOWER\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#initializing the matcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-cb879b3d1107>\u001b[0m in \u001b[0;36mmatching_with_embed\u001b[1;34m(vocab_processed, text_lines, embed_model, nlp)\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[1;31m#print('_keyword checking -->',_keyword)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                     \u001b[0mkeyword_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_keyword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                     \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_comparision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABEL_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_vocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                         \u001b[0mmatcher_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring_comparision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABEL_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-13928c8a4ae9>\u001b[0m in \u001b[0;36mchunk_comparision\u001b[1;34m(LABEL_NAME, line, nlp, embed_model, keyword_embedding, matcher_dict, keyword, line_index, updated_vocabulary)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[1;31m# print(\"\\tLine index: \", line_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_cosine_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "corpus_filename = r\"C:\\Users\\DELL\\Desktop\\NLP files\\Ner model files\\CE_GF_corpus_small_40.txt\"\n",
    "vocab_filename =r\"C:\\Users\\DELL\\Desktop\\NLP files\\Ner model files\\ESG_Keywords_Clean_4473.json\"\n",
    "matcher_dict, updated_vocabulary = CustomNerModelMainFunc(corpus_filename, vocab_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e9a9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matcher_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3727492ce4de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmatcher_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'matcher_dict' is not defined"
     ]
    }
   ],
   "source": [
    "matcher_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a583af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6febd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = r\"C:\\Users\\DELL\\Desktop\\NLP files\\Ner model files\\ESG_Keywords_Clean_4473.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7204b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(vocab_filename)\n",
    "vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {}\n",
    "for key,val in vocab.items():\n",
    "    new_vocab[key]= list(val.keys())\n",
    "    \n",
    "new_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_comparision(LABEL_NAME, line, nlp, embed_model, keyword_embedding, matcher_dict, keyword, line_index):\n",
    "    #print(\"chunk func is running\")\n",
    "    doc = nlp(line)\n",
    "    chunk_dict = {}\n",
    "    counter = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "        #print('chunk ---> ', chunk)\n",
    "        if chunk.root.text in embed_model.key_to_index:\n",
    "            similarity = get_cosine_similarity(keyword_embedding, embed_model[str(chunk.root.text)])\n",
    "            if similarity > 0.6:\n",
    "                print(\"\\tLine index: \", line_index)\n",
    "                print(\"\\tLine: \",line)\n",
    "                print(\"\\t\\t keyword: \",keyword,\"   chunk: \", chunk,\" chunk root word: \",str(chunk.root.text),\" similarity: \",similarity)\n",
    "                pattern = nlp.make_doc(str(chunk))\n",
    "                matcher_dict[LABEL_NAME].append(str(pattern))\n",
    "                counter+=1\n",
    "    return matcher_dict, counter\n",
    "\n",
    "def string_comparision(LABEL_NAME, line, keyword, matcher_dict, line_index):\n",
    "    #print('string comaprision func is running')\n",
    "    \n",
    "    match =re.search(\"\\W*(\"+keyword+\")\\W*\",line)\n",
    "    \n",
    "    if match != None:\n",
    "        print(\"\\t\\tLine index: \", line_index)\n",
    "        print(\"\\t\\tLine: \",line)\n",
    "        print(\"\\t\\t\\t keyword: \",keyword, '--- match :', match.group())\n",
    "#         if len(word_tokenize(match.group())) == 1 and match.group() in word_tokenize(line):\n",
    "#             #print(keyword, ' -----> ', match) \n",
    "#             matcher_dict[LABEL_NAME].append(str(match.group()))\n",
    "#         if len(word_tokenize(match.group())) > 1:\n",
    "            #print(keyword, ' -----> ', match) \n",
    "        matcher_dict[LABEL_NAME].append(str(match.group()))\n",
    "    return matcher_dict\n",
    "                   \n",
    "def matching_with_embed(vocab_processed, text_lines, embed_model, nlp):\n",
    "    matcher_dict = {}   \n",
    "    f_time = datetime.datetime.now()  #for saving file\n",
    "    file_time = f_time.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "    file_name = \"matcher_trial_\"+file_time+\".json\"\n",
    "    counter1 = 0\n",
    "    for LABEL_NAME in vocab_processed: #iterating through vocab\n",
    "        start_time = datetime.datetime.now()\n",
    "        matcher_dict[\"start_time\"]=start_time.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "        print(\"Currently processing keyword :\", LABEL_NAME)\n",
    "        matcher_dict[LABEL_NAME] = []\n",
    "        expanded_keywords = vocab_processed[LABEL_NAME]\n",
    "        #print('expanded_words-->', expanded_keywords[0:50])\n",
    "        for keyword in expanded_keywords[0:50]:\n",
    "            keyword = keyword.replace(' ','_')\n",
    "            #print(\"the expanded word currently running is :\", word)\n",
    "            counter1=counter1+1\n",
    "            if(counter1%10 == 0):\n",
    "                print(\"Process is running.....\", counter1)\n",
    "#             print(\"\\t------------------------------------\")\n",
    "#             print(\"\\tWordnet keywords: \",keyword)\n",
    "#            print(len(text_lines))\n",
    "            for line_index, line in enumerate(text_lines[0:2000]): #iterating through corpus lines\n",
    "                \n",
    "                #keyword_embedding=[]#np.zeros(300,)\n",
    "                if keyword in embed_model.key_to_index: #if only present in vocab\n",
    "                    keyword_embedding = embed_model[keyword]\n",
    "                    matcher_dict, counter = chunk_comparision(LABEL_NAME, line, nlp, embed_model, keyword_embedding, matcher_dict, keyword, line_index)\n",
    "                    if counter == 0:\n",
    "                        matcher_dict = string_comparision(LABEL_NAME, line, keyword, matcher_dict, line_index)\n",
    "                else:\n",
    "                    mather_dict = string_comparision(LABEL_NAME, line, keyword, matcher_dict, line_index)\n",
    "                \n",
    "            \n",
    "            \n",
    "        matcher_dict[LABEL_NAME] = list(set(matcher_dict[LABEL_NAME]))\n",
    "        end_time = datetime.datetime.now()\n",
    "        matcher_dict[\"end_time\"]=end_time.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "         \n",
    "        f =open(file_name,'a')        #Saving file for checking the vocab\n",
    "        f.write(json.dumps(list(set(matcher_dict[LABEL_NAME])))+\"\\n\")\n",
    "        f.close()\n",
    "        print(\"Keyword completed: \",LABEL_NAME)\n",
    "    return matcher_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading different embeddings function by choice #files located locally\n",
    "def load_embed(embed):\n",
    "    if embed == \"fasttext\":\n",
    "        return  (KeyedVectors.load('saved_ft_embed',mmap='r'))\n",
    "    elif embed == \"glove\":\n",
    "        return  (KeyedVectors.load('saved_glove_embed',mmap='r'))\n",
    "    elif embed == \"conceptnet\":\n",
    "        return (KeyedVectors.load(r'C:\\Users\\new\\Documents\\PythonFiles\\conceptnet_embeddings\\saved_numb_batch_embed',mmap='r'))\n",
    "    else:   \n",
    "        return None\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "\n",
    "# This function loads and preprocesses the corpus and the vocabulary\n",
    "def LoadAndPreprocessInputs(corpus_filename, vocab_filename = None, keyword_list = None):\n",
    "    print(\"preprocess functions is runnning\")\n",
    "    # Readin the corpus file\n",
    "    corpus_data = open(corpus_filename, encoding='utf-8').read().strip()\n",
    "\n",
    "    # split into patents texts | 1 entry = 1 patent\n",
    "    #corpus_texts = corpus_data.split('\\n\\n')\n",
    "\n",
    "    # split each patent into lines\n",
    "    corpus_lines = corpus_data.split('\\n')\n",
    "    print(len(corpus_lines))\n",
    "#     print(len(corpus_texts))\n",
    "#     print(type(corpus_texts))\n",
    "    #create our own vocublary using key words\n",
    "        #-------------Need to implement---------------\n",
    "        \n",
    "    if vocab_filename.endswith('json'):\n",
    "        f = open(vocab_filename)\n",
    "        vocab = json.load(f)\n",
    "        new_vocab = {}\n",
    "#         for key,val in vocab.items():\n",
    "#             new_vocab[key]= list(val.keys())\n",
    "        #print(vocab)\n",
    "        #vocab_processed = VocabPreprocess(new_vocab, corpus_texts)\n",
    "        #print(vocab_processed)\n",
    "        #print(vocab_process)\n",
    "        return corpus_lines, vocab#vocab_processed\n",
    "    # Using existing vocablary \n",
    "    if vocab_filename != None:\n",
    "        vocab = open(vocab_filename, encoding = 'utf-8').read().lower().strip().split('\\n')\n",
    "    elif keyword_list != None:\n",
    "        vocab = CreatingVocabulary(keyword_list)\n",
    "    #vocab_processed = VocabPreprocess(vocab, corpus_texts)\n",
    "    return corpus_texts, corpus_lines, vocab\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "\n",
    "#Creating vocabulary through embeddings (conceptnet, fasttext,glove) when we dont have the terms.\n",
    "def CreatingVocabulary(keyword_list):\n",
    "    concept, fasttext, glove  = LoadEmbeddings()\n",
    "    vocab_list = []\n",
    "    embed_list = [\"concept\", \"fasttext\", \"glove\"]\n",
    "\n",
    "    for embed in embed_list:\n",
    "        each_embed_vocab_list=[]\n",
    "        for keyword in keyword_list:\n",
    "            each_key_vocab = []\n",
    "            keyword_similar = embed.most_similar(keyword, topn = 5000)\n",
    "            for word in keyword_similar:\n",
    "                each_key_vocab.append(word[0].replace(\"_\", \" \"))\n",
    "                each_key_vocab.append(word[0].replace(\"-\", \" \"))\n",
    "            each_embed_vocab_list.append(each_key_vocab)\n",
    "    vocab_list.append(each_embed_vocab_list)\n",
    "    vocab_list = list(set(vocab_list))\n",
    "    return vocab_list\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "\n",
    "# This function creates possible bigrams, trigrams for the vocabulary\n",
    "def VocabPreprocess(vocab,corpus_texts):\n",
    "    if type(vocab == dict):\n",
    "        new_vocab = {}\n",
    "        print(type(corpus_texts),'---',len(corpus_texts))\n",
    "        for key in vocab:\n",
    "            cvectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=\"english\", vocabulary=vocab[key], lowercase=True)\n",
    "            X = cvectorizer.fit_transform(corpus_texts)\n",
    "            #print(X)\n",
    "            termdf_cv = pd.DataFrame(np.sum(X, axis=0), columns=cvectorizer.get_feature_names()).T.sort_values(by=0, ascending=False)\n",
    "            new_vocab[key]=termdf_cv.index\n",
    "            #print(new_vocab[key])\n",
    "        return new_vocab  \n",
    "    # Here lowercase=False option is used to keep the original case of the terms, since we possibly could have term abbreviations. Like API, CAT, etc.\n",
    "    cvectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=\"english\", vocabulary=vocab, lowercase=True)\n",
    "    X = cvectorizer.fit_transform(corpus_texts)\n",
    "\n",
    "    # Show top-25 most frequent terms\n",
    "    termdf_cv = pd.DataFrame(np.sum(X, axis=0), columns=cvectorizer.get_feature_names()).T.sort_values(by=0, ascending=False)\n",
    "    #print(termdf_cv)\n",
    "    return termdf_cv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "def matching_with_embed(vocab_processed, text, embed_model, nlp):\n",
    "    matcher_dict = {}   \n",
    "    f_time = datetime.datetime.now()\n",
    "    file_time = f_time.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "    file_name = \"matcher_trial_\"+file_time+\".json\"\n",
    "    for key in tqdm(vocab_processed):\n",
    "        start_time = datetime.datetime.now()\n",
    "        matcher_dict[\"start_time\"]=start_time.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "        print(\"Currently processing keyword :\", key)\n",
    "        matcher_dict[key] = []\n",
    "        expanded = vocab_processed[key]\n",
    "        for word in expanded:\n",
    "            #print(\"the expanded word currently running is :\", word)\n",
    "            keyword_embedding=[]#np.zeros(300,)\n",
    "            if word in embed_model.key_to_index:\n",
    "                keyword_embedding = embed_model[word]\n",
    "            \n",
    "            for line in text:\n",
    "                doc = nlp(line)\n",
    "                chunk_list = {}\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    chunk_list[str(chunk)]=str(chunk.root.text)\n",
    "                if word in line:\n",
    "                    pattern = nlp.make_doc(word)\n",
    "#                     print(line)\n",
    "#                     print(key, '---->1', pattern)\n",
    "                    matcher_dict[key].append(str(pattern))\n",
    "                elif len(keyword_embedding)>0:# != np.zeros(300,):\n",
    "                    tokens = word_tokenize(line)\n",
    "                    sent_embed = {}\n",
    "                    for tok in tokens:\n",
    "                        if tok in embed_model.key_to_index:\n",
    "                            tok_embed = embed_model[tok]\n",
    "                            sent_embed[tok]= tok_embed\n",
    "                    for emb in  sent_embed:\n",
    "                        score = get_cosine_similarity(keyword_embedding, sent_embed[emb])\n",
    "                        if score > 0.7:\n",
    "                            emb_list=[]\n",
    "                            for chun in chunk_list:\n",
    "                                if emb in chun:\n",
    "                                    pattern = nlp.make_doc(str(chun))\n",
    "#                                     print(line)\n",
    "#                                     print(key, '---->2', pattern)\n",
    "                                    matcher_dict[key].append(str(pattern))\n",
    "                                elif(not emb in emb_list):\n",
    "                                    pattern = nlp.make_doc(str(emb))\n",
    "#                                     print(line)\n",
    "#                                     print(key, '---->3', pattern)\n",
    "                                    matcher_dict[key].append(str(pattern))\n",
    "                                    emb_list.append(str(emb))\n",
    "        end_time = datetime.datetime.now()\n",
    "        matcher_dict[\"end_time\"]=end_time.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "        \n",
    "        f =open(file_name,'a')\n",
    "        f.write(json.dumps(list(set(matcher_dict[key])))+\"\\n\")\n",
    "        f.close()\n",
    "        print(\"Keyword completed: \",key)\n",
    "    return matcher_dict\n",
    "#__________________________________________________________________________________________\n",
    " # cosine similarity function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# This function processess Matcher and also splits train and test data.\n",
    "def MatcherForGeneratingTrainData(vocab_processed, corpus_lines, embed_model, LABEL_NAME):\n",
    "    print(\"matcher func is running\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    except:\n",
    "        !python -m spacy download en_core_web_lg\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    matcher_dict = matching_with_embed(vocab_processed, corpus_lines, embed_model, nlp)\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")   #initializing the matcher\n",
    "    for key, patterns in matcher_dict.items():\n",
    "        matcher.add(key, patterns)\n",
    "      \n",
    "    #splitting of corpus into train and test is done:\n",
    "    train_lines, test_lines = train_test_split(corpus_lines, test_size=0.3, random_state=42)\n",
    "    return matcher, train_lines, test_lines, nlp, matcher_dict\n",
    "\n",
    "# # This function processess Matcher and also splits train and test data.\n",
    "# def MatcherForGeneratingTrainData(vocab_processed, corpus_lines, LABEL_NAME):\n",
    "#     print(\"matcher func is running\")\n",
    "#     try:\n",
    "#         nlp = spacy.load(\"en_core_web_lg\")\n",
    "#     except:\n",
    "#         !python -m spacy download en_core_web_lg\n",
    "#         nlp = spacy.load(\"en_core_web_lg\")\n",
    "#     matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")   #initializing the matcher\n",
    "#     if type(vocab_processed) == dict:\n",
    "#       for key in vocab_processed:\n",
    "#         #print(\"key-->\",key, 'expanded--->', vocab_processed[key])\n",
    "#         patterns = [nlp.make_doc(text) for text in list(vocab_processed[key])]  #tokenization is done and converted to a list of doc objects.\n",
    "#         #print(vocab_processed)\n",
    "#         matcher.add(key, patterns)\n",
    "#     else:\n",
    "#       patterns = [nlp.make_doc(text) for text in vocab_processed]  #tokenization is done and converted to a list of doc objects.\n",
    "#       matcher.add(LABEL_NAME, patterns)\n",
    "      \n",
    "#     #splitting of corpus into train and test is done:\n",
    "#     train_lines, test_lines = train_test_split(corpus_lines, test_size=0.3, random_state=42)\n",
    "#     return matcher, train_lines, test_lines, nlp\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "def find_key(input_dict, value):\n",
    "    for key, val in input_dict.items():\n",
    "        if value in val: return key\n",
    "    return None\n",
    "\n",
    "\n",
    "# This func can create annotated dataset in .spacy format (recognizable by spacy)\n",
    "def CreateSpacyAnnotatedDataset(text, n_lines, filename, matcher, LABEL_NAME, nlp, vocab_processed, offset=0):\n",
    "    print(\"custom spacy dataset generation is running\")\n",
    "    LABEL = None\n",
    "    doc_bin = DocBin()  # create a DocBin object #serialization object\n",
    "\n",
    "    print(\"Dataset creation has begun ----->\", filename)\n",
    "    #n_lines = len(text)\n",
    "    for training_example in tqdm(text[offset:offset+n_lines]):\n",
    "        doc = nlp.make_doc(training_example)  #tokenization is done on each line and converted to a list\n",
    "        ents = []\n",
    "        #matches = matcher(doc)\n",
    "        for match_id, start, end in matcher(doc):\n",
    "            LABEL_NAME = find_key(vocab_processed, str(doc[start]))\n",
    "            print(\"document--->\",doc,\"Label_name-->\",LABEL_NAME)\n",
    "            if LABEL_NAME is not None:\n",
    "              span = Span(doc, start, end, label=LABEL_NAME)\n",
    "              print(\"document--->\",doc,\"Label_name-->\",LABEL_NAME)\n",
    "              if span is None:\n",
    "                  print(\"Skipping entity\")\n",
    "              else:\n",
    "                  ents.append(span)\n",
    "\n",
    "        filtered_ents = filter_spans(ents) #filters removes extra spans and duplicates\n",
    "        #print(len(filtered_ents))\n",
    "        #print(filtered_ents)\n",
    "        doc.ents = filtered_ents\n",
    "        doc_bin.add(doc)\n",
    "    doc_bin.to_disk(filename)\n",
    "    return filename\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "\n",
    "# This function initializes the config file and starts the training process\n",
    "def CustomNerModelTraining(train_filename, test_filename):\n",
    "    print(\"model_training is running\")\n",
    "    # Run to generate full training config\n",
    "    !python -m spacy init fill-config /content/drive/MyDrive/Colab_Files/GFCE/base_config.cfg config.cfg\n",
    "    \n",
    "    #code for training the NER\n",
    "    #code for training the NER\n",
    "    !python -m spacy train config.cfg --output /content/drive/MyDrive/Colab_Files/GFCE/spacy_output --paths.train /content/drive/MyDrive/Colab_Files/GFCE/training_data.spacy --paths.dev /content/drive/MyDrive/Colab_Files/GFCE/training_data.spacy\n",
    "    return \"/content/drive/MyDrive/Colab_Files/GFCE/spacy_output/model-best\"\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "    \n",
    "# This function is necessary if you want to test your model on a text data.\n",
    "def NerModelTesting(testing_input, model_path = \"/spacy_output/model-best\"):\n",
    "    print(\"model_testing is running\")\n",
    "    #Testing the model\n",
    "    nlp = spacy.load(model_path)\n",
    "\n",
    "    doc = nlp(testing_input)\n",
    "\n",
    "    colors = {\"CRIME\": \"#F67DE3\"}\n",
    "    options = {\"colors\": colors}\n",
    "\n",
    "    spacy.displacy.render(doc, style=\"ent\", options=options, jupyter=True)\n",
    "\n",
    "#__________________________________________________________________________________________\n",
    "\n",
    "# This is the main function\n",
    "def CustomNerModelMainFunc(corpus_filename, vocab_filename, LABEL_NAME, testing_input = None):\n",
    "    embed_model = load_embed('conceptnet')\n",
    "    corpus_texts, corpus_lines, vocab_processed = LoadAndPreprocessInputs(corpus_filename, vocab_filename)\n",
    "    matcher, train_lines, test_lines, nlp, matcher_dict = MatcherForGeneratingTrainData(vocab_processed, corpus_lines[0:100], embed_model, LABEL_NAME)\n",
    "#     train_filename = \"/content/drive/MyDrive/Colab_Files/GFCE/training_data.spacy\"\n",
    "#     test_filename = \"/content/drive/MyDrive/Colab_Files/GFCE/valid_data.spacy\"\n",
    "\n",
    "    # if not os.path.exists(train_filename):\n",
    "    #train_filename = CreateSpacyAnnotatedDataset(train_lines, len(train_lines), train_filename, matcher, LABEL_NAME, nlp,vocab_processed)\n",
    "    # if not os.path.exists(test_filename):\n",
    "    #test_filename = CreateSpacyAnnotatedDataset(test_lines, len(test_lines), test_filename, matcher, LABEL_NAME, nlp, vocab_processed)\n",
    "    #print('filenames---->',train_filename, test_filename)\n",
    "\n",
    "    #model_path = CustomNerModelTraining(train_filename, test_filename)\n",
    "#     if testing_input != None:\n",
    "#         NerModelTesting(testing_input, model_path)\n",
    "    return matcher_dict#model_path matcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a36947",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_filename = r'C:\\Users\\new\\Documents\\PythonFiles\\Hearst_pattern\\ESG_train+valid_corpus_small_150.txt'\n",
    "vocab_filename =r\"C:\\Users\\new\\Documents\\PythonFiles\\KOS algorithms and saved embeds\\whole_dictionary.json\"\n",
    "#vocab_filename = [\"rape\", \"robbery\", \"murder\", \"mutilation\", \"violence\" ]\n",
    "testing_input = \"Wi-Fi Direct (registered trademark, which will be hereinafter referred to as WFD) \\\n",
    "               corresponding to a technology for directly performing a communication based on a \\\n",
    "               wireless LAN between communication devices without intermediation of an access \\\n",
    "               point (hereinafter referred to as AP) is standardized in Wi-Fi Alliance serving \\\n",
    "               as a wireless LAN industry group. My name is Kasi working at Leadsemantics\"\n",
    "LABEL_NAME = \"INVESTMENT\"\n",
    "\n",
    "#Checking whether the files exists before the program\n",
    "if os.path.exists(corpus_filename):\n",
    "    print(\"corpus file exists\")\n",
    "else:\n",
    "    print(\"File path no found\")\n",
    "\n",
    "if type(vocab_filename) == str:\n",
    "    if os.path.exists(vocab_filename):\n",
    "        print(\"vocab file exists\")\n",
    "    else:\n",
    "        print(\"File path no found\")\n",
    "\n",
    "\n",
    "#Calling Function\n",
    "model_path = CustomNerModelMainFunc(corpus_filename, vocab_filename, LABEL_NAME, testing_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71968971",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\new\\Documents\\PythonFiles\\Hearst_pattern\\final_vocab_analysis_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3595f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(r'C:\\Users\\new\\Documents\\PythonFiles\\Hearst_pattern\\fina_vocab_dict_600.json','r')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce997f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "extended_words_list = []\n",
    "for enum, LABEL in enumerate(data):\n",
    "    for enum1,word in enumerate(data[LABEL]):\n",
    "        extended_words_list.append(word)\n",
    "        label_list.append(LABEL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199987a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extended_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_list = pd.DataFrame(\n",
    "    {'lst1Title': lst1,\n",
    "     'lst2Title': lst2,\n",
    "     'lst3Title': lst3\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({\n",
    "    'ner': extended_words_list,\n",
    "    'ner_type': label_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('ESG_ner_and_ner_type_600.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "str1=\"get together\"\n",
    "text =\"They are plannig for get togethers by sitting together.\"\n",
    "word =re.search(\"\\W*(\"+str1+\")\\W*\",text)\n",
    "print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bf79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
